一、背景：
    cache攻击就是攻击者针对cache结构，设计程序通过cache获取被攻击程序的内存访问情况，据此分析出用户的私密信息，例如用户的按键输入、秘钥等。熟悉cache结构就成为设计攻击程序或对其进行防卫的必要条件，本章主要包含CPU cache的详细介绍、cache组织、cache与内存的映射关系以及cache攻击技术等信息。
CPU caches
    当代计算机以及各种移动设备的CPU性能不仅仅依赖与其时钟周期，还受到其指令集以及和其他设备的交互的影响。因为内存应该尽可能快的向CPU提供其中存储的数据，因此如下图所示的层次图被设计出来缩小内存与CPU的速度差异。通常情况下来说，从cache中读取数据要比从内存中读取快很多倍，因此称之为缓存。
    由于访问速度越贵的内存价格越昂贵，内存架构组织成金字塔型的层次结构在访问速度和价格之间做折中，越靠近CPU的存储访问速度越快，价格也越昂贵，越靠近内存的存储价格越便宜，相应的访问速度也越慢。因此访问缓存中的数据要比访问主存中的速度要快很多。
    为了性能方面的考虑，通常情况下L1缓存直接与CPU中获取指令以及加载和存储数据的核心逻辑相连。对于冯诺依曼体系结构的计算机，通常情况下只使用一个cache用于存储指令和数据，然而哈佛体系结构的计算机则分别有一个指令缓存I-Cache和一个数据缓存D-Cache,顾名思义I-Cache用于缓存指令而D-Cache用户缓存数据，并且指令缓存和数据缓存通过两条总线可以同时传输数据，因此数据读取速度更快一些。
    程序更倾向于访问已经访问过的地址以及附近的地址，比如在一个循环中，相同的代码被一遍又一遍的执行，这也被称之为程序运行的局部性原理。因此，为了提高程序的运行速度，需要尽可能的将之后需要访问的指令及数据提早缓存到cache中，间接提高访存速度。对一些追求实时性的硬件及程序cache使得读取指令或数据的时间存在不确定性，会导致问题的发生。
    Cache仅仅缓存了主存中的一部分数据或指令，因此cache必须能够记录下这部分数据的地址以及其相关的内容。当CPU想要加载或修改某一地址的数据时，它首先去L1 cache中查找，看所需要的数据在不在缓存中。如果不在缓存中，则CPU必须到更低一级的缓存或主存中去查找数据，以确保指令流水能够顺利的执行。出自性能方面的考虑，每次会从内存中将一个内存块缓存到cache中，也就是一个cache line，其中每个内存块包含B个Bytes，这也是cache缓存的最小单位。因此当一个地址被缓存到cache中时，其附近地址的数据也被一同缓存到cache中，根据程序的局部性原理，这些数据很有可能是程序不久将要访问到的，因此能够减少从内存中获取数据频率，增加程序的运行速度。每个cache line有一个tag，表示什么地址的数据被缓存到cache中。
    全相联映射
        有很多种cache实现的方式，其中最简单的就是直接相联映射。在一个基于直接映射方式实现的cache中，主存中的每个内存块只能唯一对应cache中指定位置的set中，这种对应方式是不能改变的，也就是说主存中的数据要么只能被缓存到cache中相应的set中，要么就是仅存储在内存中。由于主存大小远远大于cache容量，因此会有多个内存块对应到同一个cache set的情况。下图展示了一个拥有128个set，每个set 64字节，总容量为4KB的cache与内存的映射情况。
        为了在cache中获取指定地址的数据，首先获取地址的index位，将地址中的tag位与cache相应index位置数据的tag位进行比较，如果tag位相等，表示cache命中，则根据offset位将响应位置的数据传输到cache中或修改改位置的数据。如果tag位不相等，则表示cache缺失，及所查找的数据不在这一级cache中，进一步将从下一级cache或从主存中获取数据到cache中，替换掉之前缓存的数据并跟新tag。
    组相联映射
        如果替换策略能够选择cache中的任何块存储数据，则这样的cache结构称为全相联映射。现在的cache被组织为若干个set，其中每个set包含固定数目的cache line，每个cache line对应内存中的一个数据块，这种组织方式被称为组相联映射。如果组相联映射结构中一个cache set包含N个cache line，则将其称之为N-way组相联。

内存与cache结构
    在过去的几十年里，得益于摩尔定律CPU的运算速度以每年大概60%的速度提升，然而内存的读写速度每年的提升幅度在7-9%之间，使得本来就慢得多的内存越来越跟不上CPU的速度，到现在两者之间已经有了很大的差距，对现在的计算机处理器，访问在一级缓存中的数据只需要0.3ns，而访问在内存中的数据需要50到150ns的时间，大约降低了2到3个数量级。为了解决CPU与内存的速度不匹配问题，现代计算机及移动设备在CPU与内存间放置多级缓存，在缓存命中的情况下大幅降低访存时间。但也引入了一些问题，及缓存缺失时会导致更长的访问时间。因此，Cache被设计为特定的访问模式以降低cache的miss几率，如全相联映射、组相联映射等，针对特定的cache模式，根据内存与cache的映射关系能够轻易的对其进行攻击，获取被攻击程序执行时的缓存信息。
    现代处理器使用一级或多级缓存的组相联缓存，从上到下内存容量逐渐增加，访问速度逐渐下降，其中每一级由S个cache set，每个cache set中包含W个cache line，每个cache line能够容纳B字节的数据，因此cache的总容量为B×W×S个字节。其中cache line是cache缓存的基本单位，当cpu读取的数据不在缓存中时，系统产生一个中断，并从内存中获取一个内存块的数据缓存到cache中，再从cache中读取数据到cpu，其中内存块block的大小与cache line的大小相同。对采用组相联映射的cache，一个cache set包含W个cache line，当cache miss发生时，从内存中获取B字节的内存块缓存到cache中，set索引号为内存块起始地址a mod [W × B]，并根据cache使用的替换策略替换掉（驱逐）set中指定的cache line。常见的替换策略由最近最少使用算法LRU、随机替换算法等，若使用LRU算法，则cache set中最近没有被访问过得cache line被驱逐到内存中去。
    现代处理器有多到3级的缓存及L1到L3，其中L1的访问速度最快，容量最小，当L1查找失败时到L2查找，相应的L2的访问速度较L1要慢，容量比L1要高，L3同理，但L1到L3的访问速度均比访问内存速度要快很多。为了简化攻击过程，本文不区分访问L1和访问L2、L3的区别，只区分cache命中和cache缺失。

