一、背景：
    cache攻击就是攻击者针对cache结构，设计程序通过cache获取被攻击程序的内存访问情况，据此分析出用户的私密信息，例如用户的按键输入、秘钥等。熟悉cache结构就成为设计攻击程序或对其进行防卫的必要条件，本章主要包含CPU cache的详细介绍、cache组织、cache与内存的映射关系以及cache攻击技术等信息。
CPU caches
    当代计算机以及各种移动设备的CPU性能不仅仅依赖与其时钟周期，还受到其指令集以及和其他设备的交互的影响。因为内存应该尽可能快的向CPU提供其中存储的数据，因此如下图所示的层次图被设计出来缩小内存与CPU的速度差异。通常情况下来说，从cache中读取数据要比从内存中读取快很多倍，因此称之为缓存。
    由于访问速度越贵的内存价格越昂贵，内存架构组织成金字塔型的层次结构在访问速度和价格之间做折中，越靠近CPU的存储访问速度越快，价格也越昂贵，越靠近内存的存储价格越便宜，相应的访问速度也越慢。因此访问缓存中的数据要比访问主存中的速度要快很多。
    为了性能方面的考虑，通常情况下L1缓存直接与CPU中获取指令以及加载和存储数据的核心逻辑相连。对于冯诺依曼体系结构的计算机，通常情况下只使用一个cache用于存储指令和数据，然而哈佛体系结构的计算机则分别有一个指令缓存I-Cache和一个数据缓存D-Cache,顾名思义I-Cache用于缓存指令而D-Cache用户缓存数据，并且指令缓存和数据缓存通过两条总线可以同时传输数据，因此数据读取速度更快一些。
    程序更倾向于访问已经访问过的地址以及附近的地址，比如在一个循环中，相同的代码被一遍又一遍的执行，这也被称之为程序运行的局部性原理。因此，为了提高程序的运行速度，需要尽可能的将之后需要访问的指令及数据提早缓存到cache中，间接提高访存速度。对一些追求实时性的硬件及程序cache使得读取指令或数据的时间存在不确定性，会导致问题的发生。
    Cache仅仅缓存了主存中的一部分数据或指令，因此cache必须能够记录下这部分数据的地址以及其相关的内容。当CPU想要加载或修改某一地址的数据时，它首先去L1 cache中查找，看所需要的数据在不在缓存中。如果不在缓存中，则CPU必须到更低一级的缓存或主存中去查找数据，以确保指令流水能够顺利的执行。出自性能方面的考虑，每次会从内存中将一个内存块缓存到cache中，也就是一个cache line，其中每个内存块包含B个Bytes，这也是cache缓存的最小单位。因此当一个地址被缓存到cache中时，其附近地址的数据也被一同缓存到cache中，根据程序的局部性原理，这些数据很有可能是程序不久将要访问到的，因此能够减少从内存中获取数据频率，增加程序的运行速度。每个cache line有一个tag，表示什么地址的数据被缓存到cache中。
    全相联映射
        有很多种cache实现的方式，其中最简单的就是直接相联映射。在一个基于直接映射方式实现的cache中，主存中的每个内存块只能唯一对应cache中指定位置的set中，这种对应方式是不能改变的，也就是说主存中的数据要么只能被缓存到cache中相应的set中，要么就是仅存储在内存中。由于主存大小远远大于cache容量，因此会有多个内存块对应到同一个cache set的情况。下图展示了一个拥有128个set，每个set 64字节，总容量为4KB的cache与内存的映射情况。
        为了在cache中获取指定地址的数据，首先获取地址的index位，将地址中的tag位与cache相应index位置数据的tag位进行比较，如果tag位相等，表示cache命中，则根据offset位将响应位置的数据传输到cache中或修改改位置的数据。如果tag位不相等，则表示cache缺失，及所查找的数据不在这一级cache中，进一步将从下一级cache或从主存中获取数据到cache中，替换掉之前缓存的数据并跟新tag。
    组相联映射
        如果替换策略能够选择cache中的任何块存储数据，则这样的cache结构称为全相联映射。现在的cache被组织为若干个set，其中每个set包含固定数目的cache line，每个cache line对应内存中的一个数据块，这种组织方式被称为组相联映射。如果组相联映射结构中一个cache set包含N个cache line，则将其称之为N-way组相联。因此，每个内存地址映射到其中一个cache set中，并且可以缓存在这个set中的任意一个cache line中。任意两个能偶映射到同一个cache set的address成为是相互关联的。关联的地址总会竞争同一个cache set中的line，并由驱逐策略决定被驱逐的line。为了方便理解，如下描述本文用于实验的Lenovo K51c78测试机的L2 cache，其cache总容量为512KB，总共包含512个set，cache与内存的映射关系为16路组相联，因此cache index位为9位。
    Cache替换策略
        在组相联结构的cache中，当cache set中每个cache line都缓存数据或当cache确实发生时，缓存的代码或数据必须从cache中驱逐到内从中以缓存新的数据或指令。决定驱逐哪个cache line的算法就被称之为替换策略。替换策略必须决定cache set中的哪些数据在将来不会被使用到。最近最少替换算法LRU总会替换掉最近没有被使用过的cache line中的数据。然而ARM结构的处理器的cache通常使用称之为pseudo-LRU伪随机替换策略，虽然也有ARM处理器使用LRU替换算法作为cache的替换策略，然而由于性能方面的考虑，在实际中通常使用伪随机替换策略。伪随机替换策略Pseudo-random将根据伪随机数生成器决定驱逐cache set中的哪个cache line。
    tag以及index
        CPU能够分别使用基于虚拟地址或物理地址的虚拟index或物理index。通常虚拟索引cache比物理索引cache要块，因为虚拟索引在cache查找时不要将虚拟地址转换为物理地址。但使用虚拟索引会导致相同的物理地址被缓存到不同的cache line中，这会导致性能的下降。为了唯一的标记缓存到指定line中的地址，通常使用tag标签来进行标记。同样，tag也能时虚拟的或者物理的。可能的搭配当时的优缺点如下所示：
        VIVT-虚拟索引，虚拟标签
            索引和标签都是使用的虚拟地址，由于在查找cache时不需要地址转换，因此查询速度较快。然而，由于虚拟标签不是唯一的，共享内存可能在cache中缓存多次
        PIPT-物理索引，物理标签
            索引和标签都是使用的物理地址，由于查找cache时需要将虚拟地址转换为物理地址，因此速度较慢，不过共享内存仅会缓存一份到cache中。
        PIVT-物理索引，虚拟标签
            物理地址用于索引，标签使用的虚拟地址，这种组合内有任何好处，因为查找时既需要将虚拟地址转换为物理地址，并且共享内存也可能在cache中缓存多份
        VIPT-虚拟索引，物理标签
            虚拟地址用于索引，物理地址用于标签，这种组合相较PIPT的优势在于通过并行查找TLB转换物理地址，然而只有当物理地址转换完毕后才能比较tag标签。
    包含性
        为了提高cache命中的几率，CPUs通常使用多级缓存L1～Ln，其中越靠近CPU的缓存速度越快，价格也越贵，容量也越小。就像先前描述的那样，本文实现基于的目标机Lenovo k51c78的cache分为两层，L1 cache结构位4路组相联，大小为32KB，共有128个sets，而L2 cache结构为16路组相联，大小位512KB，共有512个sets。此外目标机利用了哈佛体系cache结构的变种，及L1 cache分为指令cache和数据cache，但它们使用同样的地址空间。每个核均有一个L1 cache，由于k51c78共有4个核心，因此共有4个L1 cache，以及4核公用的1个L2 cache。
        如果从cache中读取一个字，则存在与cache中的该字应该与该字对应在内存中的内容相等。然而，当有核执行存储指令企图修改数据时，在写操作执行之前先要查找对应的数据是否缓存在cache中，如果cache命中，则有如下两种策略：
            写回策略：
                写会策略将数据写到cache中，因此会存在cache与内存数据不一致的问题。当没有将被写过的数据写会内存时这种方式没有什么问题，但当cache set中cache line都被占用，并且驱逐策略决定将当前cache line中的数据驱逐到内存中时，就需要判断该line中是否存有被修改过但还没有来得及写到内存中的数据。其中判断依据是以cache line的额外字段描述的，及脏位，若脏位被置为1，则表示该line存有还未写到内存中的数据，这时需要先将line中的数据写会到内存中，再写入要缓存的数据。
            写穿策略：
                写穿策略在CPU执行存储命令时将要更新的数据同时写到cache和内存中，保持主存和缓存数据的一致性，由于每次写操作都需要执行写存操作，而内存访问速度远远慢于访问cache的速度，因此执行速度较慢。
        对于包含多层缓存的处理器，cache需要决定将数据缓存到那一层中，因此有几种不同的存储策略：
            包含式缓存：
                对于低级别的缓存来说，一个高级别的缓存是包含式的，当缓存在低级别缓存中的数据同事也缓存在高级别的缓存中。因此，对于包含式缓存，低级别缓存中的数据时高级别缓存中数据的一个子集。
            排除式缓存：
                当一个cache line仅仅能保存在两级缓存的其中一级中时，该cache被成为时排除式的。
            非包含式缓存：
                如果一个缓存既不是包含式的也不是排除式的，则这种缓存结构称为是非包含式缓存。
        现代的因特尔CPUs的最后一级cache是包含式的，AMD CPUs的最后一级cache是排除式的，而大多数ARM CPUs的最后一级cache是非包含式的。然而ARM Cortex-A53/Cortex-A57 CPUs 的最后一级cache是包含式的。
    无效化以及清除cache
        当外存中的数据被修改时，需要无效化并清理缓存在cache中相应的内存，并且将陈旧的数据从cahche中删除。然而不同的平台清除缓存的方式有一些不同：
        Intel:
            Intel x86架构提供clflush指令用于将传递的地址从所有级的cache中驱逐到内存中。如果在cache的任意一级中的cache line的脏位被置为1，则在无效化cache line之前将line中的数据写回到内存中。到了第6代Intel CPUs， 引进了clflushopt指令，它和clflush有相同的功能，并且有更高的吞吐率。这两个指令都是非特权指令，因此能够在用户程序中进行调用。
        ARM：
            ARM也有类似于Intel对cache line执行清除操作的命令，并且能够cache set、cache way、虚拟地址粒度上驱逐cache中的数据。但是，这些指令只能在核心态被调用，在用户程序空间是不可用的。其无效化以及清除cache的定义如下：
            无效化：
                无效化一个cache line表示将该cache line的有效位置为0。然而，如果缓存在该cache line中的数据被修改，并且该数据还没有写回到内存，则不能仅仅使cache line无效，由于修改的数据没有写会，会导致修改的数据被丢弃。
            清除：
                如果一个cache line被清除，则脏位置为1的cache line中的内容将被写会到内存中，随后将脏位置为0，表示此时主存中的数据和cache line中的数据是一致的。该定义有效当且仅当写会策略被使用时。
    cache一致性
        若内容不够则添加
    共享内存
        共享内存是一块能够被多个程序公共访问到的内存区域，是进程间交流的方式之一，能有效的提高内存的使用率，降低内存副本的数量。虽然共享内存能够提供多个程序单核或多核处理器之间传输数据，本文主要关注基于共享库的共享内存。
        共享库或共享对象通常是一个被其他共享对象或可执行文件所分享的文件。这提供了多个程序公用相同的一组方法的能力，比如解析文件或开发网站的时候，在系统中仅仅存在相关代码的一个副本。此外，系统还提供了程序在运行时加载可执行代码到内存的函数，而不是将其链接位单个的可执行文件。共享库减少了内存的使用，因为仅有一份副本存储在内存中，降低了cache的使用，也降低了对主存以及地址转换单元的使用，因此提高了系统整体的速度。
        操作系统通过将相同的物理内存映射到不同应用的虚拟地址空间来实现共享内存，如下图所示。但是当使用自修改代码或即时编译（just-in-time）JIT时，通常就不能使用该种映射方式。通常情况下，Android应用是通过Java语言编写的，因此有可能会出现及时编译的问题，对于及时编译的代码，通常情况下时不共享的。然而也有一些方式来提高执行性能，Android运行时引擎ART能够将Java字节码编译成虚拟机二进制文件或本地代码二进制文件，而这些文件时可以被共享的。
        文件共享内存机制不关心过文件时如何打开的或如何被访问到的，且能够通过mmap系统调用将一个二进制文件映射位一个只读的共享文件。因此，即使一个应用是静态链接的，它也能够将一个共享库或任意其他可访问的二进制文件映射中的代码或数据映射到其自己的地址空间中。
        Linux和Android都提供了通过虚拟地址获取物理地址的服务，比如/proc/<pid>/maps和/proc/<pid>/pagemap保存了各个进场虚拟地址到物理地址的映射关系。Linux已经逐渐限制非特权用户访问这些文件，而Android的最新版本的操作系统也逐渐开始限制普通用户读取该服务。因此，处理器能够在非特权模式下获取到所有加载的共享文件的列表，并通过虚拟地址查找到对应的具体的物理地址。
    cache攻击
        cache边信道攻击主要利用从内存加载数据与从缓存加载数据时间上的差距泄露的信息来进行攻击的。由于读取缓存在cache中的数据要比从内存中获取数据要快很多，通常从cache中读取数据只需要不到1ns，而从内从中获取数据要几十到几百纳秒的时间，大了2个数量级，因此能够通过判断指定的数据是否缓存在cache中来推断其最近有没有被访问过。其中泄露出来的访问信息是潜在的漏洞，特别对于加密算法，因为其可能导致秘钥被破解。
        早在1996年，Kocher[43]提出了通过度量加密操作所需的总时间来破解加密系统的想法，Kocher也成为第一个提出通过CPU cache泄露的信息在不读取加密相关数据情况下破解加密系统的人。4年后Kelsey et al.[40] 提出了旁信道攻击的概念，并且断言基于cache命中率对Blowfish[71]以及CAST[14]等使用沙盒S-boxes的加密算法进行攻击时可行的。Page[64]和Tsunoo et al.[82]基于Page提出的理论对数据加密标准(DES)进行攻击。同样也有针对高级加密技术AES的cache攻击的研究，比如Bernstein[9]提出了针对AES加密技术的著名的cache-timing攻击，Neve[57]及neve et al.[59]对cache-timing进行了深入的分析。
        本节将描述几种有效的cache攻击模式，这些攻击模式在Intel x86平台上能够有效的获取cache泄露的信息，包括Evict+Time，Prime+Prime，Evict+Reload等。
        Evict+Time:
            2005年，Percival[66]和Osvik et al.[63]提出了能够跟有效的从CPU cache获取私密信息的方法，Osvik et al.还规范了这两中概念，及Evict+Time以及Prime+Probe，将在随后讨论。它们的基本思想都是判断cache中的那个或哪些set被被攻击程序访问过。
            Evict+Time算法：
                1.测量被攻击程序的执行时间
                2.驱逐cache中指定的set
                3.再次测量被攻击程序的执行时间
            Evict+Time算法可以用来决定指定的cache set在被攻击程序执行期间有没有访问到。首先，测量得到被攻击程序的执行时间t1，在下次测量被攻击程序执行时间之前，指定cache set i中的数据被驱逐到内存中，并测量得到被攻击程序的执行时间t2。最后通过比较t1与t2的区别来判断被攻击程序在执行过程中有没有访问到指定的set。若t1小于t2，则表示驱逐set i中的数据增加了被攻击程序的执行时间，也就是驱逐操作将被攻击程序之前缓存在该cache set中的数据驱逐到内存中，也就是被攻击程序执行过程中需要访问到set i。
            Osvik et al.[63]和Tromer et al.[81]提出Evict+Time能够对基于OpenSSl实现的AES实现强有力的攻击，并且在攻击过程中不需要加密过程中的明文以及密文的信息。
        Prime+Probe：
            第二种攻击模式是由Osvik et al.提出来的，并将其称之为Prime+Probe。与Evict+Time相同，通过Prime+Probe攻击者能够判断cache中的某些set有没有被被攻击程序访问到。
            Prime+Probe算法：
                1.占用指定的cache sets
                2.执行被攻击程序
                3.检测cache中的哪些sets仍然被占用
            Prime+Probe算法主要由3个步骤组成，首先，攻击程序通过其内存空间的数据占用一个或多个指定的cache set，对于使用LRU替换策略的cache，通过连续读取能够映射到指定set的与该set所包含的cache line数量相等的数据，则可将该set之前缓存的数据全部驱逐到内存中，且需要确保读取的数据在攻击程序的内存空间中。此后，执行被攻击程序，被攻击程序执行过程中的访存操作会占用cache中的某些set的某些line，并根据替换算法替换掉之前存在的部分数据。最后，攻击判断在第一阶段读取到cache中的数据是否还缓存在cache中。
            上图展示了攻击的详细过程，网格代表一个8路组相联（列）的cache，共有6个sets（行），且假设攻击者旨在检测被攻击程序在执行过程中对cache set 4的占用情况。在步骤a中，攻击程序通过连续访问能够映射到set 4中的数据来占用set 4中的8个cache line，由于该cache使用的组相联的映射方式，连续两个能够映射到该set中的地址之间的间隙位6×B字节，B为一个cache line所包含的字节数，为了将一个cache set中的所有数据清除干净，至少需要读与cache set所包含的line数相等的相关地址，本例中则至少需要读8个相关地址，记为m1,...,m8。在执行万步骤a后，cache set 4中的所有line均缓存着攻击程序的数据，由于该cache为8路组相联，因此可以将缓存在该cache set中的数据表示为b1、b2、b3、b4、b5、b6，每个字母表示一组cache line大小的数据，并由蓝色标识。
            在步骤b中，被攻击程序执行，在其执行过程中，由于不断的执行指令并访问数据，为了提高访存速度，操作系统会将这些数据所在的数据块缓存到cache中，具体缓存到那个cache set中是由数据的起始地址决定的，而具体存储到哪个cache line中是由相应的替换算法决定的。如图所示，被攻击程序执行过程中访问的部分数据对应的cache set也为set 4，且set 4中的所有line均被占用，因此cache根据替换策略替换了该set中的部分line。在这一过程中缓存的数据使用红色标识，如上图所示。
            在步骤c中，攻击程序再次访问步骤a中访问过的地址m1,...,m8，并判断这些在步骤a中缓存到cache set中的数据是否还保留在cache中。判断的依据就是当再次访问这些数据时，若访问时间快，则表示数据依然在cache中，若访问时间较慢，则表示数据在被攻击程序执行期间被驱逐到了内存中，需要再次从内存中获取数据，因此导致较长的访问时间。

        
内存与cache结构
    在过去的几十年里，得益于摩尔定律CPU的运算速度以每年大概60%的速度提升，然而内存的读写速度每年的提升幅度在7-9%之间，使得本来就慢得多的内存越来越跟不上CPU的速度，到现在两者之间已经有了很大的差距，对现在的计算机处理器，访问在一级缓存中的数据只需要0.3ns，而访问在内存中的数据需要50到150ns的时间，大约降低了2到3个数量级。为了解决CPU与内存的速度不匹配问题，现代计算机及移动设备在CPU与内存间放置多级缓存，在缓存命中的情况下大幅降低访存时间。但也引入了一些问题，及缓存缺失时会导致更长的访问时间。因此，Cache被设计为特定的访问模式以降低cache的miss几率，如全相联映射、组相联映射等，针对特定的cache模式，根据内存与cache的映射关系能够轻易的对其进行攻击，获取被攻击程序执行时的缓存信息。
    现代处理器使用一级或多级缓存的组相联缓存，从上到下内存容量逐渐增加，访问速度逐渐下降，其中每一级由S个cache set，每个cache set中包含W个cache line，每个cache line能够容纳B字节的数据，因此cache的总容量为B×W×S个字节。其中cache line是cache缓存的基本单位，当cpu读取的数据不在缓存中时，系统产生一个中断，并从内存中获取一个内存块的数据缓存到cache中，再从cache中读取数据到cpu，其中内存块block的大小与cache line的大小相同。对采用组相联映射的cache，一个cache set包含W个cache line，当cache miss发生时，从内存中获取B字节的内存块缓存到cache中，set索引号为内存块起始地址a mod [W × B]，并根据cache使用的替换策略替换掉（驱逐）set中指定的cache line。常见的替换策略由最近最少使用算法LRU、随机替换算法等，若使用LRU算法，则cache set中最近没有被访问过得cache line被驱逐到内存中去。
    现代处理器有多到3级的缓存及L1到L3，其中L1的访问速度最快，容量最小，当L1查找失败时到L2查找，相应的L2的访问速度较L1要慢，容量比L1要高，L3同理，但L1到L3的访问速度均比访问内存速度要快很多。为了简化攻击过程，本文不区分访问L1和访问L2、L3的区别，只区分cache命中和cache缺失。