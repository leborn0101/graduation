一、背景：
    cache攻击就是攻击者针对cache结构，设计程序通过cache获取被攻击程序的内存访问情况，据此分析出用户的私密信息，例如用户的按键输入、秘钥等。熟悉cache结构就成为设计攻击程序或对其进行防卫的必要条件，本章主要包含CPU cache的详细介绍、cache组织、cache与内存的映射关系以及cache攻击技术等信息。
CPU caches
    当代计算机以及各种移动设备的CPU性能不仅仅依赖与其时钟周期，还受到其指令集以及和其他设备的交互的影响。因为内存应该尽可能快的向CPU提供其中存储的数据，因此如下图所示的层次图被设计出来缩小内存与CPU的速度差异。通常情况下来说，从cache中读取数据要比从内存中读取快很多倍，因此称之为缓存。
    由于访问速度越贵的内存价格越昂贵，内存架构组织成金字塔型的层次结构在访问速度和价格之间做折中，越靠近CPU的存储访问速度越快，价格也越昂贵，越靠近内存的存储价格越便宜，相应的访问速度也越慢。因此访问缓存中的数据要比访问主存中的速度要快很多。
    为了性能方面的考虑，通常情况下L1缓存直接与CPU中获取指令以及加载和存储数据的核心逻辑相连。对于冯诺依曼体系结构的计算机，通常情况下只使用一个cache用于存储指令和数据，然而哈佛体系结构的计算机则分别有一个指令缓存I-Cache和一个数据缓存D-Cache,顾名思义I-Cache用于缓存指令而D-Cache用户缓存数据，并且指令缓存和数据缓存通过两条总线可以同时传输数据，因此数据读取速度更快一些。
    程序更倾向于访问已经访问过的地址以及附近的地址，比如在一个循环中，相同的代码被一遍又一遍的执行，这也被称之为程序运行的局部性原理。因此，为了提高程序的运行速度，需要尽可能的将之后需要访问的指令及数据提早缓存到cache中，间接提高访存速度。对一些追求实时性的硬件及程序cache使得读取指令或数据的时间存在不确定性，会导致问题的发生。
    Cache仅仅缓存了主存中的一部分数据或指令，因此cache必须能够记录下这部分数据的地址以及其相关的内容。当CPU想要加载或修改某一地址的数据时，它首先去L1 cache中查找，看所需要的数据在不在缓存中。如果不在缓存中，则CPU必须到更低一级的缓存或主存中去查找数据，以确保指令流水能够顺利的执行。出自性能方面的考虑，每次会从内存中将一个内存块缓存到cache中，也就是一个cache line，其中每个内存块包含B个Bytes，这也是cache缓存的最小单位。因此当一个地址被缓存到cache中时，其附近地址的数据也被一同缓存到cache中，根据程序的局部性原理，这些数据很有可能是程序不久将要访问到的，因此能够减少从内存中获取数据频率，增加程序的运行速度。每个cache line有一个tag，表示什么地址的数据被缓存到cache中。
    全相联映射
        有很多种cache实现的方式，其中最简单的就是直接相联映射。在一个基于直接映射方式实现的cache中，主存中的每个内存块只能唯一对应cache中指定位置的set中，这种对应方式是不能改变的，也就是说主存中的数据要么只能被缓存到cache中相应的set中，要么就是仅存储在内存中。由于主存大小远远大于cache容量，因此会有多个内存块对应到同一个cache set的情况。下图展示了一个拥有128个set，每个set 64字节，总容量为4KB的cache与内存的映射情况。
        为了在cache中获取指定地址的数据，首先获取地址的index位，将地址中的tag位与cache相应index位置数据的tag位进行比较，如果tag位相等，表示cache命中，则根据offset位将响应位置的数据传输到cache中或修改改位置的数据。如果tag位不相等，则表示cache缺失，及所查找的数据不在这一级cache中，进一步将从下一级cache或从主存中获取数据到cache中，替换掉之前缓存的数据并跟新tag。
    组相联映射
        如果替换策略能够选择cache中的任何块存储数据，则这样的cache结构称为全相联映射。现在的cache被组织为若干个set，其中每个set包含固定数目的cache line，每个cache line对应内存中的一个数据块，这种组织方式被称为组相联映射。如果组相联映射结构中一个cache set包含N个cache line，则将其称之为N-way组相联。因此，每个内存地址映射到其中一个cache set中，并且可以缓存在这个set中的任意一个cache line中。任意两个能偶映射到同一个cache set的address成为是相互关联的。关联的地址总会竞争同一个cache set中的line，并由驱逐策略决定被驱逐的line。为了方便理解，如下描述本文用于实验的Lenovo K51c78测试机的L2 cache，其cache总容量为512KB，总共包含512个set，cache与内存的映射关系为16路组相联，因此cache index位为9位。
    Cache替换策略
        在组相联结构的cache中，当cache set中每个cache line都缓存数据或当cache确实发生时，缓存的代码或数据必须从cache中驱逐到内从中以缓存新的数据或指令。决定驱逐哪个cache line的算法就被称之为替换策略。替换策略必须决定cache set中的哪些数据在将来不会被使用到。最近最少替换算法LRU总会替换掉最近没有被使用过的cache line中的数据。然而ARM结构的处理器的cache通常使用称之为pseudo-LRU伪随机替换策略，虽然也有ARM处理器使用LRU替换算法作为cache的替换策略，然而由于性能方面的考虑，在实际中通常使用伪随机替换策略。伪随机替换策略Pseudo-random将根据伪随机数生成器决定驱逐cache set中的哪个cache line。
    tag以及index
        CPU能够分别使用基于虚拟地址或物理地址的虚拟index或物理index。通常虚拟索引cache比物理索引cache要块，因为虚拟索引在cache查找时不要将虚拟地址转换为物理地址。但使用虚拟索引会导致相同的物理地址被缓存到不同的cache line中，这会导致性能的下降。为了唯一的标记缓存到指定line中的地址，通常使用tag标签来进行标记。同样，tag也能时虚拟的或者物理的。可能的搭配当时的优缺点如下所示：
        VIVT-虚拟索引，虚拟标签
            索引和标签都是使用的虚拟地址，由于在查找cache时不需要地址转换，因此查询速度较快。然而，由于虚拟标签不是唯一的，共享内存可能在cache中缓存多次
        PIPT-物理索引，物理标签
            索引和标签都是使用的物理地址，由于查找cache时需要将虚拟地址转换为物理地址，因此速度较慢，不过共享内存仅会缓存一份到cache中。
        PIVT-物理索引，虚拟标签
            物理地址用于索引，标签使用的虚拟地址，这种组合内有任何好处，因为查找时既需要将虚拟地址转换为物理地址，并且共享内存也可能在cache中缓存多份
        VIPT-虚拟索引，物理标签
            虚拟地址用于索引，物理地址用于标签，这种组合相较PIPT的优势在于通过并行查找TLB转换物理地址，然而只有当物理地址转换完毕后才能比较tag标签。
    包含性
        为了提高cache命中的几率，CPUs通常使用多级缓存L1～Ln，其中越靠近CPU的缓存速度越快，价格也越贵，容量也越小。就像先前描述的那样，本文实现基于的目标机Lenovo k51c78的cache分为两层，L1 cache结构位4路组相联，大小为32KB，共有128个sets，而L2 cache结构为16路组相联，大小位512KB，共有512个sets。此外目标机利用了哈佛体系cache结构的变种，及L1 cache分为指令cache和数据cache，但它们使用同样的地址空间。每个核均有一个L1 cache，由于k51c78共有4个核心，因此共有4个L1 cache，以及4核公用的1个L2 cache。
        如果从cache中读取一个字，则存在与cache中的该字应该与该字对应在内存中的内容相等。然而，当有核执行存储指令企图修改数据时，在写操作执行之前先要查找对应的数据是否缓存在cache中，如果cache命中，则有如下两种策略：
            写回策略：
                写会策略将数据写到cache中，因此会存在cache与内存数据不一致的问题。当没有将被写过的数据写会内存时这种方式没有什么问题，但当cache set中cache line都被占用，并且驱逐策略决定将当前cache line中的数据驱逐到内存中时，就需要判断该line中是否存有被修改过但还没有来得及写到内存中的数据。其中判断依据是以cache line的额外字段描述的，及脏位，若脏位被置为1，则表示该line存有还未写到内存中的数据，这时需要先将line中的数据写会到内存中，再写入要缓存的数据。
            写穿策略：
                写穿策略在CPU执行存储命令时将要更新的数据同时写到cache和内存中，保持主存和缓存数据的一致性，由于每次写操作都需要执行写存操作，而内存访问速度远远慢于访问cache的速度，因此执行速度较慢。
        对于包含多层缓存的处理器，cache需要决定将数据缓存到那一层中，因此有几种不同的存储策略：
            包含式缓存：
                对于低级别的缓存来说，一个高级别的缓存是包含式的，当缓存在低级别缓存中的数据同事也缓存在高级别的缓存中。因此，对于包含式缓存，低级别缓存中的数据时高级别缓存中数据的一个子集。
            排除式缓存：
                当一个cache line仅仅能保存在两级缓存的其中一级中时，该cache被成为时排除式的。
            非包含式缓存：
                如果一个缓存既不是包含式的也不是排除式的，则这种缓存结构称为是非包含式缓存。
        现代的因特尔CPUs的最后一级cache是包含式的，AMD CPUs的最后一级cache是排除式的，而大多数ARM CPUs的最后一级cache是非包含式的。然而ARM Cortex-A53/Cortex-A57 CPUs 的最后一级cache是包含式的。
    无效化以及清除cache
        当外存中的数据被修改时，需要无效化并清理缓存在cache中相应的内存，并且将陈旧的数据从cahche中删除。然而不同的平台清除缓存的方式有一些不同：

内存与cache结构
    在过去的几十年里，得益于摩尔定律CPU的运算速度以每年大概60%的速度提升，然而内存的读写速度每年的提升幅度在7-9%之间，使得本来就慢得多的内存越来越跟不上CPU的速度，到现在两者之间已经有了很大的差距，对现在的计算机处理器，访问在一级缓存中的数据只需要0.3ns，而访问在内存中的数据需要50到150ns的时间，大约降低了2到3个数量级。为了解决CPU与内存的速度不匹配问题，现代计算机及移动设备在CPU与内存间放置多级缓存，在缓存命中的情况下大幅降低访存时间。但也引入了一些问题，及缓存缺失时会导致更长的访问时间。因此，Cache被设计为特定的访问模式以降低cache的miss几率，如全相联映射、组相联映射等，针对特定的cache模式，根据内存与cache的映射关系能够轻易的对其进行攻击，获取被攻击程序执行时的缓存信息。
    现代处理器使用一级或多级缓存的组相联缓存，从上到下内存容量逐渐增加，访问速度逐渐下降，其中每一级由S个cache set，每个cache set中包含W个cache line，每个cache line能够容纳B字节的数据，因此cache的总容量为B×W×S个字节。其中cache line是cache缓存的基本单位，当cpu读取的数据不在缓存中时，系统产生一个中断，并从内存中获取一个内存块的数据缓存到cache中，再从cache中读取数据到cpu，其中内存块block的大小与cache line的大小相同。对采用组相联映射的cache，一个cache set包含W个cache line，当cache miss发生时，从内存中获取B字节的内存块缓存到cache中，set索引号为内存块起始地址a mod [W × B]，并根据cache使用的替换策略替换掉（驱逐）set中指定的cache line。常见的替换策略由最近最少使用算法LRU、随机替换算法等，若使用LRU算法，则cache set中最近没有被访问过得cache line被驱逐到内存中去。
    现代处理器有多到3级的缓存及L1到L3，其中L1的访问速度最快，容量最小，当L1查找失败时到L2查找，相应的L2的访问速度较L1要慢，容量比L1要高，L3同理，但L1到L3的访问速度均比访问内存速度要快很多。为了简化攻击过程，本文不区分访问L1和访问L2、L3的区别，只区分cache命中和cache缺失。