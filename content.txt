一、背景：
    cache攻击就是攻击者针对cache结构，设计程序通过cache获取被攻击程序的内存访问情况，据此分析出用户的私密信息，例如用户的按键输入、秘钥等。熟悉cache结构就成为设计攻击程序或对其进行防卫的必要条件，本章主要包含CPU cache的详细介绍、cache组织、cache与内存的映射关系以及cache攻击技术等信息。
CPU caches
    当代计算机以及各种移动设备的CPU性能不仅仅依赖与其时钟周期，还受到其指令集以及和其他设备的交互的影响。因为内存应该尽可能快的向CPU提供其中存储的数据，因此如下图所示的层次图被设计出来缩小内存与CPU的速度差异。通常情况下来说，从cache中读取数据要比从内存中读取快很多倍，因此称之为缓存。
    由于访问速度越贵的内存价格越昂贵，内存架构组织成金字塔型的层次结构在访问速度和价格之间做折中，越靠近CPU的存储访问速度越快，价格也越昂贵，越靠近内存的存储价格越便宜，相应的访问速度也越慢。因此访问缓存中的数据要比访问主存中的速度要快很多。
    为了性能方面的考虑，通常情况下L1缓存直接与CPU中获取指令以及加载和存储数据的核心逻辑相连。对于冯诺依曼体系结构的计算机，通常情况下只使用一个cache用于存储指令和数据，然而哈佛体系结构的计算机则分别有一个指令缓存I-Cache和一个数据缓存D-Cache,顾名思义I-Cache用于缓存指令而D-Cache用户缓存数据，并且指令缓存和数据缓存通过两条总线可以同时传输数据，因此数据读取速度更快一些。
    程序更倾向于访问已经访问过的地址以及附近的地址，比如在一个循环中，相同的代码被一遍又一遍的执行，这也被称之为程序运行的局部性原理。因此，为了提高程序的运行速度，需要尽可能的将之后需要访问的指令及数据提早缓存到cache中，间接提高访存速度。对一些追求实时性的硬件及程序cache使得读取指令或数据的时间存在不确定性，会导致问题的发生。
内存与cache结构
    在过去的几十年里，得益于摩尔定律CPU的运算速度以每年大概60%的速度提升，然而内存的读写速度每年的提升幅度在7-9%之间，使得本来就慢得多的内存越来越跟不上CPU的速度，到现在两者之间已经有了很大的差距，对现在的计算机处理器，访问在一级缓存中的数据只需要0.3ns，而访问在内存中的数据需要50到150ns的时间，大约降低了2到3个数量级。为了解决CPU与内存的速度不匹配问题，现代计算机及移动设备在CPU与内存间放置多级缓存，在缓存命中的情况下大幅降低访存时间。但也引入了一些问题，及缓存缺失时会导致更长的访问时间。因此，Cache被设计为特定的访问模式以降低cache的miss几率，如全相联映射、组相联映射等，针对特定的cache模式，根据内存与cache的映射关系能够轻易的对其进行攻击，获取被攻击程序执行时的缓存信息。
    现代处理器使用一级或多级缓存的组相联缓存，从上到下内存容量逐渐增加，访问速度逐渐下降，其中每一级由S个cache set，每个cache set中包含W个cache line，每个cache line能够容纳B字节的数据，因此cache的总容量为B×W×S个字节。其中cache line是cache缓存的基本单位，当cpu读取的数据不在缓存中时，系统产生一个中断，并从内存中获取一个内存块的数据缓存到cache中，再从cache中读取数据到cpu，其中内存块block的大小与cache line的大小相同。对采用组相联映射的cache，一个cache set包含W个cache line，当cache miss发生时，从内存中获取B字节的内存块缓存到cache中，set索引号为内存块起始地址a mod [W × B]，并根据cache使用的替换策略替换掉（驱逐）set中指定的cache line。常见的替换策略由最近最少使用算法LRU、随机替换算法等，若使用LRU算法，则cache set中最近没有被访问过得cache line被驱逐到内存中去。
    现代处理器有多到3级的缓存及L1到L3，其中L1的访问速度最快，容量最小，当L1查找失败时到L2查找，相应的L2的访问速度较L1要慢，容量比L1要高，L3同理，但L1到L3的访问速度均比访问内存速度要快很多。为了简化攻击过程，本文不区分访问L1和访问L2、L3的区别，只区分cache命中和cache缺失。

